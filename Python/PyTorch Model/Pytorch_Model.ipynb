{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxNCFjjIyy1y",
        "outputId": "80e4cb5a-16be-4629-94b1-9336438ed0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# mnist_cnn.py\n",
        "# PyTorch 1.10.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
        "# Windows 10 \n",
        "\n",
        "# reads MNIST data from text file rather than using\n",
        "# built-in black box Dataset from torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as T\n",
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "device = T.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "\n",
        "class MNIST_Dataset(T.utils.data.Dataset):\n",
        "  # 784 tab-delim pixel values (0-255) then label (0-9)\n",
        "  def __init__(self, src_file):\n",
        "    all_xy = np.loadtxt(src_file, usecols=range(785),\n",
        "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
        "\n",
        "    tmp_x = all_xy[:, 0:784]  # all rows, cols [0,783]\n",
        "    tmp_x /= 255\n",
        "    tmp_x = tmp_x.reshape(-1, 1, 28, 28)  # bs, chnls, 28x28\n",
        "    tmp_y = all_xy[:, 784]    # 1-D required\n",
        "\n",
        "    self.x_data = \\\n",
        "      T.tensor(tmp_x, dtype=T.float32).to(device)\n",
        "    self.y_data = \\\n",
        "      T.tensor(tmp_y, dtype=T.int64).to(device) \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    lbl = self.y_data[idx]  # no use labels\n",
        "    pixels = self.x_data[idx] \n",
        "    return (pixels, lbl)"
      ],
      "metadata": {
        "id": "A9H1jmzQ0Mj_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "\n",
        "class Net(T.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()  # pre Python 3.3 syntax\n",
        "\n",
        "    \"\"\"self.conv1 = T.nn.Conv2d(1, 32, 5)  # chnl-in, out, krnl\n",
        "    self.conv2 = T.nn.Conv2d(32, 64, 5)\n",
        "    self.fc1 = T.nn.Linear(1024, 512)   # [64*4*4, x]\n",
        "    self.fc2 = T.nn.Linear(512, 256)\n",
        "    self.fc3 = T.nn.Linear(256, 10)     # 10 classes\n",
        "    self.pool1 = T.nn.MaxPool2d(2, stride=2) # kernel, stride\n",
        "    self.pool2 = T.nn.MaxPool2d(2, stride=2)\n",
        "    self.drop1 = T.nn.Dropout(0.25)\n",
        "    self.drop2 = T.nn.Dropout(0.50)\"\"\"\n",
        "\n",
        "    self.conv1 = T.nn.Conv2d(1, 1, 5)  # chnl-in, out, krnl\n",
        "    self.conv2 = T.nn.Conv2d(32, 64, 5)\n",
        "    #self.fc1 = T.nn.Linear(1024, 512)   # [64*4*4, x]\n",
        "    #self.fc2 = T.nn.Linear(512, 256)\n",
        "    #self.fc3 = T.nn.Linear(256, 10)     # 10 classes\n",
        "    self.fc1 = T.nn.Linear(144, 10)   # [64*4*4, x]\n",
        "    #self.fc2 = T.nn.Linear(512, 256)\n",
        "    #self.fc3 = T.nn.Linear(256, 10)     # 10 classes\n",
        "    self.pool1 = T.nn.MaxPool2d(2, stride=2) # kernel, stride\n",
        "    self.pool2 = T.nn.MaxPool2d(2, stride=2)\n",
        "    self.drop1 = T.nn.Dropout(0.25)\n",
        "    self.drop2 = T.nn.Dropout(0.50)\n",
        "\n",
        "  def print_everything(self):\n",
        "\n",
        "    print(self.conv1.data)\n",
        "    print(self.conv2)\n",
        "    print(self.fc1)\n",
        "    print(self.fc2)\n",
        "    print(self.fc3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # convolution phase         # x is [bs, 1, 28, 28]\n",
        "    \"\"\"z = T.relu(self.conv1(x))   # Size([bs, 32, 24, 24])\n",
        "    z = self.pool1(z)           # Size([bs, 32, 12, 12])\n",
        "    z = self.drop1(z)\n",
        "    z = T.relu(self.conv2(z))   # Size([bs, 64, 8, 8])\n",
        "    z = self.pool2(z)           # Size([bs, 64, 4, 4])\n",
        "   \n",
        "    # neural network phase\n",
        "    z = z.reshape(-1, 1024)     # Size([bs, 1024])\n",
        "    z = T.relu(self.fc1(z))     # Size([bs, 512])\n",
        "    z = self.drop2(z)\n",
        "    z = T.relu(self.fc2(z))     # Size([bs, 256])\n",
        "    z = self.fc3(z)             # Size([bs, 10])\"\"\"\n",
        "\n",
        "    # convolution phase         # x is [bs, 1, 28, 28]\n",
        "    z = T.relu(self.conv1(x))   # Size([bs, 1, 24, 24])\n",
        "    z = self.pool1(z)           # Size([bs, 1, 12, 12])\n",
        "    #z = self.drop1(z)\n",
        "    #z = T.relu(self.conv2(z))   # Size([bs, 64, 8, 8])\n",
        "    #z = self.pool2(z)           # Size([bs, 64, 4, 4])\n",
        "   \n",
        "    # neural network phase\n",
        "    z = z.reshape(-1, 144)     # Size([bs, 144])\n",
        "    #z = T.relu(self.fc1(z))     # Size([bs, 10])\n",
        "    z = self.fc1(z)     # Size([bs, 10])\n",
        "    #z = self.drop2(z)\n",
        "    #z = T.relu(self.fc2(z))     # Size([bs, 256])\n",
        "    #z = self.fc3(z)             # Size([bs, 10])\n",
        "    return z"
      ],
      "metadata": {
        "id": "9-MaQGiY0QeC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, ds):\n",
        "  ldr = T.utils.data.DataLoader(ds,\n",
        "    batch_size=len(ds), shuffle=False)\n",
        "  n_correct = 0\n",
        "  for data in ldr:\n",
        "    (pixels, labels) = data\n",
        "    with T.no_grad():\n",
        "      oupts = model(pixels)\n",
        "    (_, predicteds) = T.max(oupts, 1)\n",
        "    n_correct += (predicteds == labels).sum().item()\n",
        "    print(predicteds)\n",
        "\n",
        "  acc = (n_correct * 1.0) / len(ds)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "Qx67qS8g0VkU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # 0. setup\n",
        "  print(\"\\nBegin MNIST with CNN demo \")\n",
        "  np.random.seed(1)\n",
        "  T.manual_seed(1)"
      ],
      "metadata": {
        "id": "BL3SgCQ30Wi8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # 1. create Dataset\n",
        "  print(\"\\nCreating 1000-item train Dataset from text file \")\n",
        "  train_file = \"./sample_data/mnist_train_1000.txt\"\n",
        "  train_ds = MNIST_Dataset(train_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODA2IyYY0liE",
        "outputId": "22bc344c-08d0-44c7-894b-8fd339eecb64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating 1000-item train Dataset from text file \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  bat_size = 10\n",
        "  train_ldr = T.utils.data.DataLoader(train_ds,\n",
        "    batch_size=bat_size, shuffle=True)\n",
        "\n",
        "  # 2. create network\n",
        "  print(\"\\nCreating CNN network with 1 conv and 1 linear \")\n",
        "  net = Net().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMntIMVW17nS",
        "outputId": "cebb8107-3bdd-4a37-e940-c87487bb5812"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating CNN network with 1 conv and 1 linear \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 3. train model\n",
        "  max_epochs = 50  # 100 gives better results\n",
        "  ep_log_interval = 5\n",
        "  lrn_rate = 0.005\n",
        "  \n",
        "  loss_func = T.nn.CrossEntropyLoss()  # does log-softmax()\n",
        "  optimizer = T.optim.SGD(net.parameters(), lr=lrn_rate)\n",
        "  \n",
        "  print(\"\\nbat_size = %3d \" % bat_size)\n",
        "  print(\"loss = \" + str(loss_func))\n",
        "  print(\"optimizer = SGD\")\n",
        "  print(\"max_epochs = %3d \" % max_epochs)\n",
        "\n",
        "  print(\"lrn_rate = %0.3f \" % lrn_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgn0XxOQ23dB",
        "outputId": "a07da8b7-35d4-4028-8352-60c43eb73e43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bat_size =  10 \n",
            "loss = CrossEntropyLoss()\n",
            "optimizer = SGD\n",
            "max_epochs =  50 \n",
            "lrn_rate = 0.005 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  print(\"\\nStarting training\")\n",
        "  net.train()  # set mode\n",
        "  for epoch in range(0, max_epochs):\n",
        "    ep_loss = 0  # for one full epoch\n",
        "    for (batch_idx, batch) in enumerate(train_ldr):\n",
        "      (X, y) = batch  # X = pixels, y = target labels\n",
        "      optimizer.zero_grad()\n",
        "      oupt = net(X)\n",
        "      loss_val = loss_func(oupt, y)  # a tensor\n",
        "      #print(\"The output is \")\n",
        "      #print(oupt)\n",
        "      #print(y)\n",
        "      ep_loss += loss_val.item()  # accumulate\n",
        "      loss_val.backward()  # compute grads\n",
        "      optimizer.step()     # update weights\n",
        "    if epoch % ep_log_interval == 0:\n",
        "      print(\"epoch = %4d   loss = %0.4f\" % (epoch, ep_loss))\n",
        "\n",
        "  print(\"Done \") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-SbuNcH26J_",
        "outputId": "896ea32a-2796-4a28-a07c-2f04744a3fe9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training\n",
            "epoch =    0   loss = 230.3140\n",
            "epoch =    5   loss = 146.8176\n",
            "epoch =   10   loss = 48.9457\n",
            "epoch =   15   loss = 37.3201\n",
            "epoch =   20   loss = 32.0552\n",
            "epoch =   25   loss = 29.0123\n",
            "epoch =   30   loss = 26.0886\n",
            "epoch =   35   loss = 24.4442\n",
            "epoch =   40   loss = 22.4409\n",
            "epoch =   45   loss = 20.7928\n",
            "Done \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 4. evaluate model accuracy\n",
        "  print(\"\\nComputing model accuracy\")\n",
        "  net.eval()\n",
        "  #T.set_printoptions(linewidth=10)\n",
        "  print(\"The convolution weight is : \")\n",
        "  print(net.conv1.weight)\n",
        "  print(\"The linear weight is : \")\n",
        "  #print(loaded_matrix)\n",
        "  print(net.fc1.weight)\n",
        "  weight_matrix = net.fc1.weight.T.detach().numpy()\n",
        "  for i in range(144):\n",
        "    for j in range(10):\n",
        "        print(weight_matrix[i][j], end=' ')\n",
        "    print()\n",
        "  acc_train = accuracy(net, train_ds)  # all at once\n",
        "\n",
        "  print(\"Accuracy on training data = %0.4f\" % acc_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmz71wuQ3nqT",
        "outputId": "8df628d0-6f3f-4dc6-d1c4-8730369b37c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing model accuracy\n",
            "The convolution weight is : \n",
            "Parameter containing:\n",
            "tensor([[[[ 0.3914,  0.3111,  0.7927,  0.7223,  0.9370],\n",
            "          [ 0.2425,  0.4552,  0.8965,  0.9039,  1.0564],\n",
            "          [-0.1454,  0.3218,  0.6565,  0.8154,  0.6088],\n",
            "          [-0.1639,  0.1987,  0.3324,  0.1958,  0.2762],\n",
            "          [-0.3905, -0.0256,  0.1656,  0.0082, -0.0591]]]], requires_grad=True)\n",
            "The linear weight is : \n",
            "Parameter containing:\n",
            "tensor([[ 0.0759, -0.0605,  0.0498,  ...,  0.0070, -0.0454, -0.0520],\n",
            "        [-0.0402,  0.0287, -0.0433,  ..., -0.0223, -0.0551, -0.0084],\n",
            "        [ 0.0120, -0.0228, -0.0687,  ...,  0.0653, -0.0020, -0.0703],\n",
            "        ...,\n",
            "        [-0.0458, -0.0200,  0.0353,  ..., -0.1573, -0.0829,  0.0136],\n",
            "        [-0.0637, -0.0162,  0.0690,  ...,  0.0735, -0.0811,  0.0314],\n",
            "        [ 0.0171, -0.0140, -0.0300,  ...,  0.1415,  0.0760,  0.0286]],\n",
            "       requires_grad=True)\n",
            "0.07585539 -0.0401862 0.011950545 -0.054809995 -0.0005036357 0.010673035 -0.059212316 -0.04583555 -0.063678384 0.01714766 \n",
            "-0.06046958 0.028705392 -0.02275636 -0.07282279 0.033375017 0.076094784 0.086462915 -0.019955471 -0.016205011 -0.014004562 \n",
            "0.04979761 -0.043285962 -0.068720825 0.07829619 -0.02734451 0.039175242 -0.025406437 0.035314843 0.06904711 -0.029963108 \n",
            "-0.025239224 0.025850603 -0.023244092 0.01577918 -0.091012746 -0.021515237 0.02354661 0.046960134 -0.023328725 -0.06995902 \n",
            "-0.122653596 0.035257597 -0.031864032 0.04063642 0.03269946 0.046129096 0.09464734 -0.073153794 0.023392607 0.0141311 \n",
            "-0.068414286 0.08214599 0.11919941 0.06999045 -0.013807626 -0.025390543 0.10652706 0.01284735 -0.06297703 -0.03158306 \n",
            "0.04177287 -0.06575089 -0.051372953 -0.006181826 0.09715532 -0.07762081 0.04386794 0.036558874 0.058165453 -0.022183314 \n",
            "0.062227268 -0.057902936 -0.06781267 0.0659388 -0.009619409 -0.06082993 0.14332028 0.03359816 0.020675635 0.023596697 \n",
            "0.00023048831 -0.0006096269 0.00881116 -0.07245211 -0.054105476 -0.09633886 0.11381204 -0.08610867 -0.06785373 -0.029862257 \n",
            "-0.04022906 0.054196857 0.09154095 0.043566328 0.055812813 -0.024432793 -0.011463367 -0.009367281 -0.03968055 -0.06815232 \n",
            "-0.08549906 -0.003399455 -0.036698613 -0.03478584 0.006620722 -0.0013879149 -0.044363357 0.034209058 -0.041030526 0.033230897 \n",
            "0.037676375 -0.030651841 0.05839997 0.07420391 -0.07760718 -0.02176041 0.069170594 -0.010409156 -0.06366067 0.05600931 \n",
            "0.056118805 0.046694055 0.05519003 -0.035090175 -0.0023822933 -0.001960623 -0.08761758 -0.034093183 0.069616064 -0.009191022 \n",
            "-0.007911551 0.085896395 -0.014890116 0.051705536 0.027004836 -0.06621878 -0.015167681 -0.0141529925 -0.031757977 -0.024310807 \n",
            "0.022900416 0.0507532 -0.00787852 0.1069271 0.048349284 -0.012395812 0.09769446 -0.05391523 0.016781379 -0.08845758 \n",
            "-0.14407893 -0.043222573 -0.02310914 0.06177932 0.030431956 0.019351594 0.042506244 -0.08100025 -0.13762303 0.033299167 \n",
            "-0.07869373 -0.06340138 -0.021220734 -0.06224096 -0.051008772 -0.032301355 0.20909002 -0.07005155 -0.007987682 -0.069547184 \n",
            "-0.06847757 0.056545787 0.092066854 0.0027375238 -0.038601488 0.08295919 0.027140697 -0.06494025 -0.10486133 0.02259883 \n",
            "-0.007964444 0.0043026097 0.021493651 0.024163337 0.0840935 -0.045995638 0.13594067 -0.022305248 -0.036042906 -0.14037018 \n",
            "0.059533805 0.04688196 0.041694436 -0.032685593 0.054003425 -0.10560042 0.18489686 -0.07274829 0.062525816 -0.08396685 \n",
            "0.0029524495 0.05375678 0.12987207 0.059387848 0.037137464 -0.054095272 0.15397985 -0.013485113 0.021927236 -0.04514592 \n",
            "-0.03651879 0.08794826 0.03289694 -0.028849324 0.10405763 0.008364673 0.14367051 0.045227 0.03138535 -0.08736701 \n",
            "0.028752392 0.021092137 0.070058405 -0.022918472 0.03609943 -0.019326137 0.039831083 -0.07959797 0.07751804 -0.06908682 \n",
            "0.068166465 -0.03851008 -0.015178952 0.060854923 0.06093202 -0.060504634 0.053604003 -0.044887234 -0.0665638 -0.055673 \n",
            "0.008111979 -0.022926817 0.12499555 0.0898858 0.014566059 -0.07270659 0.021888062 0.02572837 0.04141368 -0.07383777 \n",
            "-0.06646207 0.012637882 0.1486358 0.21069764 -0.070443764 -0.08386129 0.049114466 -0.141191 0.011090638 -0.06906804 \n",
            "0.07970612 -0.018817862 0.103559785 0.07254585 -0.020494549 0.12600681 -0.020590797 -0.042914025 -0.120836645 -0.15622604 \n",
            "-0.052670907 -0.035334647 0.051432945 0.15872866 0.03997329 0.035320956 0.036057483 0.008928562 -0.06225288 0.0070552677 \n",
            "0.016177483 -0.112404205 0.12634435 0.042750284 0.049472935 -0.016557977 -0.0249622 0.06015987 -0.08661562 0.0021909939 \n",
            "0.16042444 -0.06417809 -0.0068805837 -0.015678842 -0.015129778 -0.03868923 -0.07397565 -0.020602275 0.08195257 -0.04214006 \n",
            "0.09018103 -0.06577204 -0.0041978345 -0.016467636 -0.1525194 -0.1164228 -0.026605306 -0.013574849 0.1720996 0.11050426 \n",
            "0.06659231 0.031169323 0.0035188883 0.045016136 -0.021843143 -0.03570908 -0.100217834 -0.032640595 0.068408005 -0.05187801 \n",
            "-0.01468951 0.1003131 0.084557556 0.07765424 -0.052652925 0.07969143 0.010161232 -0.2281396 0.004129693 -0.03673781 \n",
            "-0.021611702 0.1716667 0.05011088 -0.044035543 0.14744444 0.07162804 -0.06524889 -0.2145865 0.05333648 -0.06869647 \n",
            "0.057760287 -0.02208719 -0.08130101 -0.05514121 0.22747524 0.10479596 -0.09551291 -0.022906356 0.11261854 -0.120684795 \n",
            "-0.045882642 -0.020812165 -0.1107139 -0.043323524 -0.03070524 0.029218182 -0.027582102 -0.07589395 0.07987895 -0.03052531 \n",
            "0.063668095 0.054452255 0.089061216 0.03293028 -0.017260293 -0.10135776 -0.016728 0.034459725 0.020839393 -0.10029974 \n",
            "-0.03421211 -0.052697036 0.07795773 0.12037637 0.041813318 -0.01778932 -0.017679768 0.052174993 0.037852798 -0.070254676 \n",
            "-0.021828007 -0.01610033 0.03366532 0.10531184 0.0033785857 0.12175632 -0.11825791 -0.0009780264 0.07709457 -0.1318598 \n",
            "-0.07465314 -0.045096822 0.07745563 0.16622937 -0.018501377 -0.014068767 -0.1282243 0.06228563 -0.0007718979 -0.021192601 \n",
            "-0.09186653 -0.11027781 -0.014400803 0.045403596 0.0035217607 0.023631422 -0.076699525 0.082435705 0.079360895 0.029590031 \n",
            "0.026710877 -0.1165742 0.101145096 -0.0102966605 -0.2407235 0.0314927 -0.04675701 0.07442978 -0.0444767 0.20901966 \n",
            "0.08815973 -0.12383676 -0.021793317 0.08977867 -0.21200086 0.068799846 -0.12374222 0.17370565 -0.14981306 0.20676917 \n",
            "0.14871241 -0.14190097 0.11399085 0.07857685 -0.10678238 0.01438739 -0.13945557 0.12961271 -0.090718344 0.09720655 \n",
            "0.2148209 -0.032920416 -0.010158395 -0.09056814 -0.1237775 0.065170966 -0.16728671 -0.022935381 0.04252184 0.118910715 \n",
            "-0.0007821777 0.00900222 0.03487809 0.08377795 0.06124037 0.034929432 -0.115742154 -0.12338793 -0.025058335 -0.14277288 \n",
            "0.03814071 0.015168501 -0.1335916 0.08199985 0.012698853 0.3112559 -0.032571018 -0.048442014 0.074389376 -0.12701058 \n",
            "-0.040054295 -0.090248086 -0.07215134 -0.072754145 0.08807573 0.15006655 -0.1316562 0.07990785 0.042778846 -0.17780975 \n",
            "0.047391895 0.029560277 -0.0059078583 0.067240246 0.014131434 0.030505003 -0.0045802305 0.10538117 -0.0027226007 -0.01746121 \n",
            "0.112302735 -0.07762866 -0.06080286 -0.012290712 -0.016926724 -0.07405985 -0.10525643 0.046494417 0.043592855 0.0051865857 \n",
            "-0.07403224 -0.028265895 0.049810052 -0.13591678 -0.09716994 0.03072998 0.086182535 0.060993697 0.08447267 -0.009597263 \n",
            "-0.10017136 -0.0064435075 -0.12015495 -0.18638909 -0.047481667 0.017037602 -0.01257048 0.19316036 0.10274049 0.12536295 \n",
            "0.004164578 0.01928858 -0.1233756 -0.2280408 -0.027781894 0.17930084 -0.112694785 -0.012704005 0.07917441 0.0657957 \n",
            "0.017283363 0.18799199 -0.098271616 0.005999546 -0.13395429 0.046968315 0.028208805 0.06216898 -0.06650593 0.010840253 \n",
            "0.0019719845 0.15884537 0.020610934 0.2715104 -0.15265594 -0.081570745 -0.1014536 0.3220912 -0.08647825 -0.052115563 \n",
            "0.00290442 0.046418007 0.048993014 0.20633546 -0.037026756 -0.16963664 -0.17864051 0.28279278 0.019287566 0.02347425 \n",
            "0.09234493 -0.09988881 0.07533183 0.116960056 -0.08315307 -0.12745342 -0.17138387 0.24249992 -0.022480238 0.13015863 \n",
            "0.067125954 -0.05329782 0.06250963 0.024704145 -0.031334512 -0.06386569 -0.029256677 0.06280983 0.07260529 0.13312799 \n",
            "0.03608207 -0.11630469 -0.11383612 0.086413585 -0.101899795 0.14603251 -0.03855838 -0.034374185 0.14180861 -0.043384224 \n",
            "0.071957454 0.0050753085 -0.042534985 -0.087556124 -0.109646834 0.14987327 -0.06704224 -0.05163087 0.029603815 -0.12176462 \n",
            "-0.027117599 0.06682586 -0.12479072 -0.021593336 0.05305235 -0.004655399 -0.07925142 0.03316892 0.10618097 0.022550007 \n",
            "0.010918911 0.056466796 -0.2328573 -0.1354093 -0.03587258 -0.009728458 -0.031766526 -0.07043556 0.07709573 0.05447522 \n",
            "-0.060921673 0.06664437 -0.07605312 -0.19617495 0.14038034 -0.09789407 0.1716793 0.077916406 0.043982707 0.13646647 \n",
            "0.059627097 -0.08903387 -0.19109902 -0.15752606 0.18638323 0.09395466 0.073816724 0.11293232 -0.0363103 0.1681671 \n",
            "0.13795146 -0.11371269 -0.4264885 -0.06681701 0.14253682 0.1368489 0.084321454 -0.1369766 0.08765367 0.017771171 \n",
            "-0.022017954 0.17407374 -0.2522926 0.0045573064 -0.089450024 0.07478456 -0.102271184 -0.06360403 0.14696594 -0.08204717 \n",
            "-0.2310124 0.23857692 -0.16283984 0.16234665 0.024029108 -0.08961253 -0.15607499 0.177834 0.049821332 -0.051827647 \n",
            "-0.12118484 -0.087083556 -0.0227321 0.0070690457 -0.035409644 -0.066846035 -0.26048106 0.17637427 -0.044119637 0.17625098 \n",
            "-0.038780525 -0.17473182 -0.043276858 0.11002372 0.00075989985 -0.11071294 -0.020844905 0.23713145 0.009707209 0.12095751 \n",
            "0.10738641 -0.14097835 -0.04032904 0.06902232 -0.010898387 -0.20615825 0.06928176 0.14069 0.13990764 0.07697097 \n",
            "0.12307024 0.031816408 -0.079329014 0.040689737 0.033656996 -0.11015133 0.13385275 0.037206557 0.07865876 -0.043628614 \n",
            "-0.004087063 -0.08170238 -0.05360481 -0.036375895 -0.11529306 -0.017888807 -0.044813886 0.009488989 -0.051481955 -0.054982383 \n",
            "0.019270226 0.070810735 -0.102286525 -0.04520291 0.028275378 0.019130984 0.018619973 0.06020947 -0.030402841 0.10244765 \n",
            "0.03345665 0.13318706 -0.19264096 -0.169375 -0.006557897 -0.06522211 -0.094349675 -0.06961544 0.009948689 0.07706349 \n",
            "0.016471844 0.04210711 -0.06342401 -0.14260235 0.09749907 -0.042009305 0.04292947 -0.023571199 -0.1704615 -0.013098434 \n",
            "0.059994902 -0.28395444 -0.056538053 0.06541111 0.22785635 0.13053042 0.112007886 -0.15606906 -0.14827594 0.009711267 \n",
            "0.004395114 -0.17327292 -0.05477702 0.08859016 0.117290035 0.08819435 -0.021210562 -0.2994151 -0.023244219 -0.03102288 \n",
            "-0.25817496 0.26248285 -0.024511278 0.15242058 -0.10024218 0.059140243 0.016525107 -0.31065556 0.12343741 -0.040965796 \n",
            "-0.2759191 0.21347994 -0.09022375 0.017720079 0.07557115 -0.019851375 0.08818028 -0.098894 0.16004312 0.105729215 \n",
            "-0.14653543 -0.15864904 -0.09293326 0.026084833 0.1952457 -0.03350506 0.055925746 -0.03728166 -0.010481805 0.31997102 \n",
            "0.06429012 -0.15593107 -0.10243561 0.060690757 0.047298327 0.07732123 -0.0064372974 0.12026779 -0.09376423 0.0069025108 \n",
            "0.050438527 -0.042026762 -0.121391766 -0.021111604 0.0021717565 -7.6688586e-05 0.1900503 0.050292376 -0.14186274 0.031621173 \n",
            "0.12730266 0.05042761 -0.09524517 0.11051467 -0.044481866 -0.056234524 0.1560862 -0.040924523 0.06374456 0.0217762 \n",
            "0.105871335 0.05761314 -0.059162393 0.011009259 -0.07246539 -0.040987357 0.052777473 -0.07131039 0.04536491 -0.07049718 \n",
            "0.021642966 0.034679726 -0.031887174 -0.04641815 0.027899228 -0.067004815 0.043218985 -0.03780607 -0.051734798 0.003322217 \n",
            "0.037437215 -0.11278403 0.030205917 -0.0051017483 0.08825244 -0.041543514 -0.09489337 0.07459293 -0.06323766 -0.041509952 \n",
            "0.027711472 -0.13627762 -0.010331309 -0.07237183 0.15750094 -0.055271473 0.0028991215 0.04702907 -0.24639775 0.010173671 \n",
            "0.15270643 -0.2728978 0.048839662 -0.040124793 0.075103015 -0.04852132 0.16762865 -0.011064729 0.043765858 0.050855357 \n",
            "-0.02510468 0.10463249 0.110965 0.06728347 0.030431233 -0.026123537 0.20347758 -0.19326209 0.08437626 0.015007535 \n",
            "-0.31207937 0.13865477 0.076144636 -0.11844386 0.027033877 -0.0007237899 0.088322826 -0.11782559 -0.08632718 -0.14844875 \n",
            "-0.24564932 -0.0090257395 -0.08491551 -0.062168743 0.21041682 -0.012385859 -0.016244583 -0.0625497 0.06965207 0.05180115 \n",
            "0.01778677 -0.18854164 -0.08385472 0.12147159 0.20697154 0.0024708062 -0.069370896 0.0507584 -0.0037472676 0.05550825 \n",
            "0.03760307 -0.088458955 0.027230265 0.037917968 0.06711839 0.016340317 0.11543498 0.1453502 -0.1018508 -0.07006147 \n",
            "-0.056299537 -0.045859084 0.02880252 0.005855172 0.102030024 0.038366422 0.07172394 0.008836752 -0.14060582 -0.06359049 \n",
            "0.109855115 -0.053553823 0.06509429 0.050074805 0.069431104 0.060454324 -0.022083933 -0.04919368 0.017793478 -0.09471572 \n",
            "0.07974495 0.028672699 -0.01702827 0.0497013 0.04368079 -0.039232362 -0.092194445 -0.04986021 -0.060452998 0.0017543443 \n",
            "0.0053562294 -0.043617796 0.12218191 0.06155809 -0.030915307 0.027295252 -0.06633123 -0.07935101 -0.020087609 0.0018348683 \n",
            "0.068235956 -0.048484653 0.14791803 0.11457158 0.0335609 0.068116404 -0.06502875 -0.066864274 -0.05608947 0.007695457 \n",
            "0.19587924 -0.15743434 -0.023303054 0.018163199 0.08396087 0.16192721 0.05228769 -0.035964396 0.06570365 -0.043427467 \n",
            "0.15804681 -0.091984205 -0.053613152 -0.16001086 0.004950595 -0.020120705 0.2097574 0.01789745 0.21470635 -0.006259433 \n",
            "0.015213609 0.053392235 0.14674103 -0.25448093 -0.06635609 -0.24379107 0.2366181 -0.055952717 0.14497721 -0.01052704 \n",
            "-0.20939219 0.20550285 0.1904929 -0.21031718 0.13170543 -0.18592273 -0.0064250627 0.108727396 -0.07616243 -0.014621658 \n",
            "-0.11586186 0.04426223 0.018563827 -0.12843962 0.10388487 -0.10133043 -0.036196847 0.103772916 0.08043989 0.071029454 \n",
            "0.015860487 -0.14781709 0.086177394 0.034879915 -0.04400367 0.12540144 -0.02838237 0.06574331 0.006665537 -0.07015282 \n",
            "-0.02148687 0.05097764 0.14227113 -0.06220243 -0.069016695 0.03708615 -0.080466844 -0.03132422 -0.041210692 -0.108469956 \n",
            "0.049916934 -0.028555425 0.18407153 -0.010891078 -0.09418695 0.037485767 0.03415808 -0.17944634 0.057181872 -0.14144683 \n",
            "0.018201083 -0.06724637 0.21484281 0.034226004 -0.043792278 0.026481966 -0.032924153 -0.13897738 -0.054030765 -0.11913046 \n",
            "-0.0038477227 -0.008590049 0.13131173 0.11744998 0.018355569 0.054841734 0.0023142202 -0.036251828 0.02864732 -0.0076923426 \n",
            "0.078981645 -0.01320578 0.103005104 0.13619077 -0.09978098 -0.059425883 -0.122196816 -0.036221728 -0.07399308 0.07967253 \n",
            "-0.011927224 -0.040634148 0.1446947 0.16656716 -0.20048721 0.1912409 -0.035053484 -0.101230055 0.071170874 0.05679835 \n",
            "0.05130579 0.0475081 0.124449655 -0.052941687 -0.08604474 0.22135283 0.04096756 -0.099510655 0.03136055 -0.11354665 \n",
            "0.073041745 -0.0018896761 0.089574814 -0.2433818 -0.15188134 0.04972997 0.16287392 -0.09424688 0.09527563 -0.1492837 \n",
            "0.07778668 0.124228835 0.11507693 -0.22224458 -0.03889582 -0.012372019 0.14643158 -0.08279126 -0.15407047 -0.15265052 \n",
            "0.04835711 -0.013310639 0.16561241 -0.10402496 0.067376345 0.039856113 0.100187935 -0.10166797 -0.10535192 -0.049694926 \n",
            "-0.051131148 0.0080461465 0.1599729 0.08968883 0.100435786 0.093943916 -0.0058371224 -0.10101129 -0.13731964 -0.03564511 \n",
            "-0.07098143 0.08457569 0.15789652 0.15064172 -0.087636426 0.10463437 0.06905439 -0.19379494 -0.10951383 -0.07426415 \n",
            "-0.062389094 0.077031285 0.048797887 0.047467463 -0.08105367 0.023671152 0.034096826 -0.1832444 0.0723015 0.037249923 \n",
            "-0.111450106 0.056827556 0.006994104 0.11301178 -0.026112633 0.016722156 -0.023951141 -0.06900515 0.08075184 -0.007230227 \n",
            "-0.06357055 -0.11681074 0.1508968 0.014105679 -0.03204575 0.07136459 0.035527505 -0.07492892 0.030223811 0.040358115 \n",
            "-0.08304081 -0.09300657 -0.008348486 0.0077362643 0.06495696 -0.08863224 -0.06447344 -0.029141914 -0.029894242 0.014608592 \n",
            "0.04182655 -0.07717134 0.041675895 0.11831213 -0.066406995 -0.04646416 0.010057193 -0.11266996 -0.09445374 -0.031361535 \n",
            "-0.02821385 0.06351328 0.15043847 0.034711547 -0.004762238 -0.12179406 -0.09450727 -0.017512018 -0.002501718 0.027285596 \n",
            "0.09780115 0.091638744 0.05739865 0.015751462 -0.083542526 -0.015042842 -0.018194787 -0.03464904 0.04561015 -0.016111147 \n",
            "0.07021026 0.08465627 -0.040044565 0.022592148 -0.140593 0.0764538 0.03974167 -0.020028284 -0.07807749 -0.13189997 \n",
            "0.026000012 -0.080931365 0.11746818 -0.06163763 -0.1430577 0.17904186 0.12019642 -0.08888939 0.028151521 -0.040921826 \n",
            "0.15812458 -0.06795708 0.039959155 0.012115544 -0.11257748 -0.032240957 0.02809801 -0.024642736 0.0006069535 -0.09554073 \n",
            "0.038649607 0.036776718 -0.023327833 0.14211026 0.05776566 -0.027416302 0.016290577 -0.12524976 0.05678173 -0.04939473 \n",
            "-0.0946114 0.11932457 0.034216397 0.07176984 -0.025500732 0.006023757 0.09729517 -0.17608851 0.010641221 -0.07513731 \n",
            "-0.06303595 0.04565017 0.0063029244 0.054024924 0.120776795 -0.048399318 -0.008343936 -0.08958555 0.048490494 -0.09380554 \n",
            "0.0031453422 0.04674643 0.013035691 -3.98701e-05 0.08264415 -0.08539903 0.01919442 -0.12917507 0.07769542 0.001775097 \n",
            "-0.05069178 0.008910383 0.090630256 0.060724787 -0.072611205 -0.0048957034 -0.07770039 -0.02449879 -0.028382361 0.08080936 \n",
            "0.073270746 0.05292864 0.049977243 0.074593686 0.044553537 -0.08646138 0.06703397 -0.045316465 -0.06521317 -0.01232736 \n",
            "0.047939513 -0.002750197 -0.014354717 0.11185486 0.06999141 -0.012378784 -0.027144073 -0.011249933 -0.1339528 -0.10413467 \n",
            "0.025626373 0.0269582 -0.02108633 0.1937343 -0.05192051 -0.08219629 -0.034429036 -0.016141385 -0.087257765 -0.029890671 \n",
            "0.020841416 0.12066496 -0.008248005 0.090251565 -0.014846081 -0.104180574 -0.12864836 0.14654426 -0.13208567 -0.029178366 \n",
            "0.0704923 0.04136685 -0.098467626 0.04713088 -1.2967229e-05 -0.034657706 -0.14541624 0.12273612 0.071024 0.0046804375 \n",
            "0.062373664 -0.12802702 -0.06520394 0.016088406 -0.031512823 -0.032134894 -0.19276418 0.13187023 0.09140258 0.0001241805 \n",
            "0.13139312 -0.09059308 -0.13737473 -0.089285545 0.02352914 0.040209778 -0.084567115 0.043236036 0.10425167 -0.0693525 \n",
            "-0.025113467 -0.08094597 -0.027825864 -0.117685586 -0.052593675 0.11575116 -0.08005277 0.1377981 0.14810926 0.017413452 \n",
            "-0.014012666 0.120828934 0.05306148 -0.007512463 0.058133785 -0.049778584 -0.09574181 -0.047923587 0.04981322 -0.0049865143 \n",
            "0.032668445 0.093265526 0.1452554 -0.008160806 0.085160695 -0.0015108304 0.020210251 -0.054004345 -0.066350594 0.040194876 \n",
            "0.0069956267 -0.022334982 0.06528615 -0.08686409 -0.0439965 -0.039132554 0.028102813 -0.15726143 0.07345392 0.14153388 \n",
            "-0.04541959 -0.055105947 -0.0019875092 0.043300997 0.00072095566 -0.011049883 -0.002162858 -0.082890846 -0.08106556 0.075976945 \n",
            "-0.051973995 -0.008365329 -0.07030972 -0.0589632 0.0518918 0.012977044 0.029351013 0.013593668 0.031415384 0.028571071 \n",
            "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
            "        1, 2, 4, 3, 2, 7, 3, 8, 6, 7, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
            "        5, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 2, 7,\n",
            "        1, 6, 3, 0, 2, 1, 1, 7, 8, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
            "        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
            "        2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 1, 9, 3, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
            "        8, 5, 8, 6, 9, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
            "        7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
            "        0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 9, 2, 7, 3,\n",
            "        0, 4, 6, 5, 2, 6, 4, 7, 9, 8, 9, 9, 5, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
            "        8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3, 6, 4, 7, 5, 0, 6, 2, 7,\n",
            "        4, 8, 5, 9, 7, 1, 1, 4, 4, 5, 6, 4, 1, 2, 6, 3, 9, 3, 9, 0, 5, 9, 6, 5,\n",
            "        7, 4, 1, 3, 4, 0, 4, 8, 0, 4, 3, 6, 8, 7, 6, 0, 9, 7, 5, 7, 2, 1, 1, 6,\n",
            "        8, 9, 4, 1, 3, 2, 2, 9, 0, 3, 9, 6, 7, 2, 0, 3, 5, 4, 3, 6, 5, 8, 9, 5,\n",
            "        4, 7, 4, 2, 9, 3, 4, 8, 9, 1, 9, 2, 8, 7, 9, 1, 8, 7, 4, 1, 3, 1, 1, 0,\n",
            "        2, 3, 9, 4, 9, 2, 1, 6, 8, 4, 7, 7, 4, 4, 4, 2, 5, 7, 2, 4, 4, 2, 1, 9,\n",
            "        7, 2, 8, 7, 6, 9, 2, 2, 3, 8, 1, 6, 5, 1, 1, 0, 2, 6, 4, 5, 8, 3, 1, 5,\n",
            "        1, 9, 2, 7, 4, 4, 9, 8, 1, 5, 8, 9, 5, 6, 7, 9, 9, 3, 7, 0, 9, 0, 6, 6,\n",
            "        2, 3, 9, 0, 7, 5, 4, 8, 0, 9, 4, 1, 1, 8, 7, 1, 2, 6, 1, 0, 3, 0, 1, 1,\n",
            "        8, 2, 0, 9, 9, 4, 0, 5, 0, 6, 1, 7, 7, 8, 1, 9, 2, 0, 5, 1, 2, 2, 7, 3,\n",
            "        5, 4, 9, 7, 1, 8, 3, 9, 6, 0, 3, 1, 1, 2, 0, 3, 5, 7, 6, 8, 2, 9, 5, 1,\n",
            "        5, 7, 6, 1, 1, 3, 1, 7, 5, 5, 5, 2, 5, 8, 7, 0, 9, 7, 7, 5, 0, 9, 0, 0,\n",
            "        8, 9, 2, 4, 8, 1, 6, 1, 6, 5, 1, 8, 3, 4, 0, 5, 5, 8, 3, 6, 2, 3, 9, 2,\n",
            "        1, 1, 5, 2, 1, 3, 2, 8, 7, 3, 7, 2, 4, 6, 9, 7, 2, 4, 2, 8, 1, 1, 3, 8,\n",
            "        4, 0, 6, 5, 9, 3, 0, 9, 2, 4, 7, 1, 2, 9, 4, 2, 6, 1, 8, 9, 0, 6, 6, 7,\n",
            "        9, 9, 8, 0, 1, 4, 4, 6, 7, 1, 5, 7, 0, 3, 5, 8, 4, 7, 1, 2, 5, 9, 5, 6,\n",
            "        7, 5, 7, 6, 8, 3, 6, 9, 7, 0, 7, 2, 7, 1, 1, 0, 7, 9, 2, 3, 7, 3, 2, 4,\n",
            "        1, 6, 2, 7, 5, 5, 7, 4, 0, 2, 6, 3, 6, 4, 0, 4, 2, 6, 0, 0, 0, 0, 3, 1,\n",
            "        6, 2, 2, 3, 1, 4, 1, 5, 4, 6, 4, 7, 2, 8, 7, 9, 2, 0, 5, 1, 4, 2, 8, 3,\n",
            "        2, 4, 1, 5, 4, 6, 0, 7, 9, 8, 4, 9, 8, 0, 1, 1, 0, 2, 2, 3, 2, 4, 4, 5,\n",
            "        1, 6, 5, 7, 7, 8, 8, 9, 7, 4, 7, 3, 2, 0, 8, 6, 8, 6, 1, 6, 8, 9, 4, 0,\n",
            "        9, 0, 4, 1, 5, 9, 7, 5, 3, 7, 9, 9, 8, 5, 8, 6, 5, 8, 6, 9, 9, 1, 8, 3,\n",
            "        5, 8, 2, 5, 9, 7, 2, 5, 0, 8, 4, 1, 1, 0, 9, 1, 8, 6, 7, 0, 9, 3, 0, 8,\n",
            "        8, 9, 6, 7, 8, 4, 7, 5, 9, 4, 6, 7, 4, 5, 9, 2, 3, 1, 6, 3, 9, 2, 2, 5,\n",
            "        6, 8, 0, 7, 7, 1, 9, 8, 7, 0, 9, 9, 4, 6, 2, 8, 5, 1, 4, 1, 8, 5, 1, 7,\n",
            "        3, 6, 4, 3, 2, 5, 5, 4, 4, 0, 4, 4, 6, 7, 9, 4, 3, 3, 8, 0, 0, 3, 2, 2,\n",
            "        7, 8, 2, 3, 7, 0, 1, 1, 0, 2, 3, 3, 8, 4, 3, 8, 7, 6, 4, 7, 2, 8, 5, 9,\n",
            "        7, 0, 3, 1, 6, 2, 4, 3, 4, 4, 7, 5, 9, 6, 0, 0, 7, 1, 4, 2, 7, 3, 6, 7,\n",
            "        5, 8, 4, 5, 5, 2, 7, 1, 1, 5, 6, 8, 5, 8, 4, 0, 7, 9, 9, 2, 9, 4, 7, 8,\n",
            "        7, 4, 2, 6, 9, 1, 7, 0, 6, 4, 8, 5, 7, 0, 7, 1, 0, 3, 7, 6, 5, 0, 6, 1,\n",
            "        5, 1, 7, 8, 5, 0, 3, 4, 7, 7, 5, 7, 8, 6, 9, 3, 8, 6, 1, 0, 9, 7, 1, 3,\n",
            "        0, 5, 6, 4, 4, 2, 4, 4, 3, 1, 7, 4, 6, 0, 3, 6])\n",
            "Accuracy on training data = 0.9580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  test_file = \"./sample_data/mnist_test_100.txt\"\n",
        "  test_ds = MNIST_Dataset(test_file)\n",
        "  net.eval()\n",
        "  acc_test = accuracy(net, test_ds)  # all at once\n",
        "  print(\"Accuracy on test data = %0.4f\" % acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AePP9Elw4GbO",
        "outputId": "daa056e7-3f3c-489a-beb4-df00863bc8ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 7, 1, 1, 3, 9, 4, 3, 4, 8, 2, 2, 1, 8, 7, 0, 8, 1, 0, 0, 6, 3, 7, 7,\n",
            "        4, 8, 8, 9, 2, 0, 6, 1, 7, 2, 2, 9, 9, 4, 9, 5, 0, 6, 7, 7, 1, 8, 0, 7,\n",
            "        2, 0, 4, 1, 1, 2, 7, 5, 9, 7, 8, 8, 1, 9, 5, 8, 8, 1, 9, 8, 3, 1, 1, 5,\n",
            "        7, 4, 2, 4, 7, 3, 0, 1, 1, 1, 7, 0, 1, 1, 8, 5, 9, 5, 0, 6, 6, 0, 4, 1,\n",
            "        2, 2, 4, 4])\n",
            "Accuracy on test data = 0.8300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 5. make a prediction\n",
        "  print(\"\\nMaking prediction for fake image: \")\n",
        "  x = np.zeros(shape=(28,28), dtype=np.float32)\n",
        "  for row in range(5,23):\n",
        "    x[row][9] = 180  # vertical line\n",
        "  for rc in range(9,19):\n",
        "    x[rc][rc] = 250  # diagonal\n",
        "  for col in range(5,15):  \n",
        "    x[14][col] = 200  # horizontal\n",
        "  x /= 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZF_trEz5Iw-",
        "outputId": "8b829777-53d9-4c21-9d40-cca871118ddf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making prediction for fake image: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  plt.tight_layout()\n",
        "  plt.imshow(x, cmap=plt.get_cmap('gray_r'))\n",
        "  plt.show()\n",
        "\n",
        "  digits = ['zero', 'one', 'two', 'three', 'four', 'five', \n",
        "    'six', 'seven', 'eight', 'nine' ]\n",
        "  x = x.reshape(1, 1, 28, 28)  # 1 image, 1 channel\n",
        "  x = T.tensor(x, dtype=T.float32).to(device)\n",
        "  with T.no_grad():\n",
        "    oupt = net(x)  # 10 logits like [[-0.12, 1.03, . . ]]\n",
        "  am = T.argmax(oupt) # 0 to 9\n",
        "  print(\"\\nPredicted class is \\'\" + digits[am] + \"\\'\")\n",
        "\n",
        "  # 6. save model\n",
        "  print(\"\\nSaving trained model state\")\n",
        "  fn = \"mnist_model.pt\"\n",
        "  T.save(net.state_dict(), fn)  \n",
        "\n",
        "  print(\"\\nEnd MNIST CNN demo \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "_xYI9qB65O_b",
        "outputId": "66d26bfe-1a61-4e10-e872-3157b14f83a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class is 'four'\n",
            "\n",
            "Saving trained model state\n",
            "\n",
            "End MNIST CNN demo \n"
          ]
        }
      ]
    }
  ]
}